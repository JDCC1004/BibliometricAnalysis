@incollection{ANDERSON1996237,
title = {Chapter 8 - Interference and Inhibition in Memory Retrieval},
editor = {Elizabeth Ligon Bjork and Robert A. Bjork},
booktitle = {Memory},
publisher = {Academic Press},
address = {San Diego},
pages = {237-313},
year = {1996},
isbn = {978-0-12-102570-0},
doi = {https://doi.org/10.1016/B978-012102570-0/50010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780121025700500100},
author = {Michael C. Anderson and James H. Neely},
abstract = {Publisher Summary
This chapter discusses the causes of memory interference and the extent of situations in which these mechanisms operate. First, the chapter discusses some widely held assumptions about the situation of interference, focusing on the idea that such effects arise from competition for access via a shared retrieval cue. This notion is sufficiently general that it may be applied in a variety of interference settings, which is illustrated briefly. Then the classical interference paradigms from which these ideas emerged are reviewed. The chapter also reviews more recent phenomena that both support and challenge classical conceptions of interference. These phenomena provide compelling illustrations of the generality of interference and, consequently, of the importance of understanding its mechanisms. A recent perspective on interference is highlighted that builds upon insights from modern work, while validating intuitions underlying several of the classical interference mechanisms. According to this new perspective, forgetting derives not from acquiring new memories per se, but from the impact of later retrievals of the newly learned material. After discussing findings from several paradigms that support this retrieval-based view, the chapter illustrates how forgetting might be linked to inhibitory processes underlying selective attention.}
}
@article{HSIAO2021102312,
title = {Who captures whom – Pokémon or tourists? A perspective of the Stimulus-Organism-Response model},
journal = {International Journal of Information Management},
volume = {61},
pages = {102312},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102312},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000050},
author = {Chun-Hua Hsiao and Kai-Yu Tang},
keywords = {Stimulus-Organism-Response (S-O-R) model, Critical mass, Social interaction, Attachment, Conformity, Tourism, Continuance intention},
abstract = {Since its launch in 2016, Pokémon Go has attracted huge numbers of players, causing a boom in this game market. Although it is not as popular as before, from time to time we still find crowds of players gathered in some spots where Pokémon appear. Numerous reports have explored this Pokémon phenomenon; however, the exact reasons for its popularity remain unknown. The purpose of this study is to explore the post-adoption behavior of Pokémon Go players and its influential factors in the gaming and tourism industries. The theoretical model of stimulus-organism-response was drawn on to examine the impact of the environmental stimuli (social influence and media influence) on players’ internal organisms, which in turn affect their post-experience responses. Moreover, gender differences were also examined in the hypothetical relationships. A total of 342 valid questionnaires from actual gamers were collected in this study, and data analysis was performed using a structural equation model. The results show that stimulus effects, such as social stimuli (critical mass and social interaction) and media stimuli (content timeliness and media richness), have significant impacts on the players’ internal gamified experience (attachment and conformity), which in turn affect their visit intention to catch creatures at certain attractions and to continue playing Pokémon Go. Further, we have also found that players’ intention to visit Pokémon spots is significantly correlated with their intention to continue playing the game. Findings provide links between gamification and tourism literature. Further theoretical and managerial implications are provided.}
}
@incollection{SEYMOURE2021315,
title = {Chapter 18 - Conservation behavior: effects of light pollution on insects},
editor = {Heather Zimbler-DeLorenzo and Susan W. Margulis},
booktitle = {Exploring Animal Behavior in Laboratory and Field (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {315-335},
year = {2021},
isbn = {978-0-12-821410-7},
doi = {https://doi.org/10.1016/B978-0-12-821410-7.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128214107000017},
author = {Brett Seymoure and Elizabeth K. Peterson and Rachel Y. Chock},
keywords = {Activity, Conservation behavior, Foraging, Light pollution, Madagascar hissing cockroach},
abstract = {In this experiment, we will study the impact of light pollution on the activity and foraging behaviors using Madagascar hissing cockroaches as the model system. You will develop your own hypotheses using the background information provided. You will then test your hypotheses, evaluate the results of your experiment, and revise and retest your experiment up to three times.}
}
@article{CHEN2020100002,
title = {Application and theory gaps during the rise of Artificial Intelligence in Education},
journal = {Computers and Education: Artificial Intelligence},
volume = {1},
pages = {100002},
year = {2020},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2020.100002},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X20300023},
author = {Xieling Chen and Haoran Xie and Di Zou and Gwo-Jen Hwang},
keywords = {Artificial intelligence in education, Systematic review, Application gap, Theory gap},
abstract = {Considering the increasing importance of Artificial Intelligence in Education (AIEd) and the absence of a comprehensive review on it, this research aims to conduct a comprehensive and systematic review of influential AIEd studies. We analyzed 45 articles in terms of annual distribution, leading journals, institutions, countries/regions, the most frequently used terms, as well as theories and technologies adopted. We also evaluated definitions of AIEd from broad and narrow perspectives and clarified the relationship among AIEd, Educational Data Mining, Computer-Based Education, and Learning Analytics. Results indicated that: 1) there was a continuingly increasing interest in and impact of AIEd research; 2) little work had been conducted to bring deep learning technologies into educational contexts; 3) traditional AI technologies, such as natural language processing were commonly adopted in educational contexts, while more advanced techniques were rarely adopted, 4) there was a lack of studies that both employ AI technologies and engage deeply with educational theories. Findings suggested scholars to 1) seek the potential of applying AI in physical classroom settings; 2) spare efforts to recognize detailed entailment relationships between learners’ answers and the desired conceptual understanding within intelligent tutoring systems; 3) pay more attention to the adoption of advanced deep learning algorithms such as generative adversarial network and deep neural network; 4) seek the potential of NLP in promoting precision or personalized education; 5) combine biomedical detection and imaging technologies such as electroencephalogram, and target at issues regarding learners’ during the learning process; and 6) closely incorporate the application of AI technologies with educational theories.}
}
@article{CHIU2013142,
title = {WISEngineering: Supporting precollege engineering design and mathematical understanding},
journal = {Computers & Education},
volume = {67},
pages = {142-155},
year = {2013},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2013.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0360131513000936},
author = {Jennifer L. Chiu and Peter T. Malcolm and Deborah Hecht and Crystal J. DeJaegher and Edward A. Pan and Michael Bradley and M. David Burghardt},
keywords = {Evaluation of CAL systems, Improving classroom teaching, Interactive learning environments, Interdisciplinary projects, Multimedia/hypermedia systems},
abstract = {Introducing engineering into precollege classroom settings has the potential to facilitate learning of science, technology, engineering, and mathematics (STEM) concepts and to increase interest in STEM careers. Successful engineering design projects in secondary schools require extensive support for both teachers and students. Computer-based learning environments can support both teachers and students to implement and learn from engineering design projects. However, there is a dearth of empirical research on how engineering approaches can augment learning in authentic K-12 settings. This paper presents research on the development and pilot testing of WISEngineering, a new web-based engineering design learning environment. Three middle school units were developed using a knowledge integration learning perspective and a scaffolded, informed engineering approach with the goal of improving understanding of standards-based mathematical concepts and engineering ideas. Seventh grade math students from two teachers in a socioeconomically diverse and low-performing district participated in three WISEngineering units over the course of a semester. Students significantly improved their mathematical scores from pretest to posttest for all three projects and on state standardized tests. Student, teacher, and administrator interviews reveal that WISEngineering projects promoted collaboration, tolerance, and development of pro-social skills among at-risk youth. Results demonstrate that informed engineering design projects facilitated through the WISEngineering computer-based environment can help students learn Common Core mathematical concepts and principles. Additionally, results suggest that WISEngineering projects can be particularly beneficial for at-risk and diverse student populations.}
}
@article{FINKEL2017303,
title = {L’analyse cognitive, la psychologie numérique et la formation des enseignants à l’université},
journal = {Pratiques Psychologiques},
volume = {23},
number = {3},
pages = {303-323},
year = {2017},
note = {Préparer la nouvelle génération de psychologues : objectifs, méthodes et ressources dans l'enseignement de la psychologie},
issn = {1269-1763},
doi = {https://doi.org/10.1016/j.prps.2017.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S126917631730055X},
author = {A. Finkel},
keywords = {Pensée informatique, Psychologie numérique, Analyse cognitive, Enseignement, Université, Digital thinking, Computer science, Digital psychology, Cognitive analysis, Teaching, College, University},
abstract = {Résumé
L’analyse cognitive est le fruit d’une réflexion utilisant, entre autres, les principes de la pensée informatique (mathématique, logique et algorithmique) pour repenser des connaissances et des techniques de plusieurs domaines de la psychologie cognitive, de la psychologie sociale, de la psychologie des émotions et de la psychologie de la communication en vue de la formation pédagogique des enseignants d’université. Avant de présenter l’analyse cognitive, je vais rappeler comment l’histoire du calcul depuis 5000 ans peut être vue comme une tentative (réussie) de comprendre, formaliser et automatiser le traitement humain de l’information, thème sur lequel se retrouvent l’informatique et la psychologie. Puis, je montrerai qu’un nouveau comportementalisme, basé sur le comportement numérique mais aussi sur les comportements classiques et sur les zones du cerveau, est en train d’émerger et qu’il produit des modèles psychologiques qui sont utilisés pour connaître les pensées et les intentions des personnes observées, pour mieux leur vendre des objets et pour prédire leurs actions par exemple ; une psychologie numérique émerge. Bien que chacun d’entre nous utilise son ordinateur portable et internet comme une extension de son esprit (lire « Petite Poucette » de Michel Serres), nous n’avons pas encore intégré explicitement la pensée informatique. Nous expliciterons donc quelques éléments de la pensée informatique et nous présenterons enfin l’analyse cognitive en soulignant comment elle utilise à la fois la pensée informatique et des connaissances de psychologie.
Cognitive analysis is the result of an afterthought using, amongst other things, principles from computer science (maths, logic and algorithmic) to rethink techniques and knowledge of several areas of psychology: cognitive psychology, social psychology, psychology of emotions and communication. The aim is pedagogy training for university teachers. Before explaining what cognitive analysis is, I will tell you how the history of calculation since 5000 years may be seen as a (successful) stab at understanding, formalizing and automating human processing of data, which is where computer science and psychology meet. Then, I will demonstrate that a new behavioralism, based on digital and classical behavior and specific areas of the brain, is starting to emerge and produces psychological models that are used to know the thoughts and intents of the people observed, to better sell them stuff and predict their actions. A digital psychology emerges. Although all of us use a laptop and Internet as an extension of our minds (read “Petite Poucette” by Michel Serres), the digital thought remains implicit. We will explicit several components of digital thinking and will show how cognitive analysis uses digital thinking and knowledge from psychology.}
}
@article{BEDEWY2023101299,
title = {STEAM + X - Extending the transdisciplinary of STEAM-based educational approaches: A theoretical contribution},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101299},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101299},
url = {https://www.sciencedirect.com/science/article/pii/S187118712300069X},
author = {Shereen El Bedewy and Zsolt Lavicza},
keywords = {STEAM, Design-based research, Culture, Technology, Design principles},
abstract = {This design-based research methodological paper is proposing a theoretical understanding in the form of STEAM + X framework that emerged from the empirical findings of implementing transdisciplinary STEAM practices featuring architecture, culture, and history. This paper shows how the proposed STEAM practices, involving creativities, to promote the integration of various disciplines with multiple cross-cultural iterations. These STEAM practices allow teachers to integrate cultural, architectural, environmental, or technological options into mathematics teaching and learning. These STEAM practices foster creativity and thinking skills in connecting disciplines in a transdisciplinary learning approach. Moreover, this paper introduces the study outcomes including the developed design principles and a framework that connects the underlying theoretical framework with emerging themes from our qualitative data analysis.}
}
@article{PROFETA2018111,
title = {Bernstein’s levels of movement construction: A contemporary perspective},
journal = {Human Movement Science},
volume = {57},
pages = {111-133},
year = {2018},
issn = {0167-9457},
doi = {https://doi.org/10.1016/j.humov.2017.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167945717305717},
author = {Vitor L.S. Profeta and Michael T. Turvey},
keywords = {Bernstein, Control, Coordination, Synergy, Movement construction},
abstract = {Explanation of how goal-directed movements are made manifest is the ultimate aim of the field classically referred to as “motor control”. Essential to the sought-after explanation is comprehension of the supporting functional architecture. Seven decades ago, the Russian physiologist and movement scientist Nikolai A. Bernstein proposed a hierarchical model to explain the construction of movements. In his model, the levels of the hierarchy share a common language (i.e., they are commensurate) and perform complementing functions to bring about dexterous movements. The science of the control and coordination of movement in the phylum Craniata has made considerable progress in the intervening seven decades. The contemporary body of knowledge about each of Bernstein’s hypothesized functional levels is both more detailed and more sophisticated. A natural consequence of this progress, however, is the relatively independent theoretical development of a given level from the other levels. In this essay, we revisit each level of Bernstein’s hierarchy from the joint perspectives of (a) the ecological approach to perception-action and (b) dynamical systems theory. We review a substantial and relevant body of literature produced in different areas of study that are accommodated by this ecological-dynamical version of Bernstein’s levels. Implications for the control and coordination of movement and the challenges to producing a unified theory are discussed.}
}
@article{GRAY2019124,
title = {BrainQuest: The use of motivational design theories to create a cognitive training game supporting hot executive function},
journal = {International Journal of Human-Computer Studies},
volume = {127},
pages = {124-149},
year = {2019},
note = {Strengthening gamification studies: critical challenges and new opportunities},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918304555},
author = {Stuart Iain Gray and Judy Robertson and Andrew Manches and Gnanathusharan Rajendran},
keywords = {Gamification, Motivational theory, Game design, Cognitive training games, Executive functions},
abstract = {For children to yield greater mental performance abilities in real world settings, training approaches should offer practice in problems which have an affective component requiring social interactions, and be motivating over a sustained period. Current cognitive training games often overlook the important relationship between cognition and emotion, characterised by ‘hot executive function’, and correlated with fundamental academic and life outcomes. Here, we present robust qualitative evidence from a case study which documents the social relationships, motivation and engagement of a class of ten-year-old children who used an active smartphone cognitive training game called BrainQuest in their physical education lessons over a period of 5 weeks. Game design elements which are intended to move beyond simple gamification of cognitive tests are presented, along with a discussion of how these design elements worked in practice. The paper also presents and discusses the impact of the game upon the cognitive and emotional regulatory skills, characterised by executive function skills, based on the findings of this initial work. We conclude with recommendations for the designers of cognitive training games in the future and discussion of appropriate research methods for future gamification studies.}
}
@article{IIVARI2022100408,
title = {Critical agenda driving child–computer interaction research—Taking a stock of the past and envisioning the future},
journal = {International Journal of Child-Computer Interaction},
volume = {32},
pages = {100408},
year = {2022},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100408},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000957},
author = {Netta Iivari and Sumita Sharma and Leena Ventä-Olkkonen and Tonja Molin-Juustila and Kari Kuutti and Jenni Holappa and Essi Kinnunen},
keywords = {Critical, Critical research, Critical design, Critical agenda, Children},
abstract = {There is a revitalized interest in power and politics around design and technology in the Human–Computer Interaction (HCI) field. Child–Computer Interaction (CCI) research community has also shown arousing interest towards the topic. However, despite this emerging interest, the CCI research community has remained quite silent about the potential of a critical agenda for CCI. Few studies have explicitly addressed critical research or critical design. This study introduces the notion of a critical agenda for CCI research and identifies CCI studies that are linked with the critical agenda, revealing that there are CCI studies showing emerging interests and seeds for addressing the critical agenda. Overall, this study explores the state-of-the-art critical research tradition in CCI and explicates the potential of this tradition for making the world a better place through design and technology in collaboration with children.}
}
@article{ESTEFO2019226,
title = {The Robot Operating System: Package reuse and community dynamics},
journal = {Journal of Systems and Software},
volume = {151},
pages = {226-242},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300342},
author = {Pablo Estefo and Jocelyn Simmonds and Romain Robbes and Johan Fabry},
keywords = {Robot Operating System, Package management, Software ecosystems},
abstract = {ROS, the Robot Operating System, offers a core set of software for operating robots that can be extended by creating or using existing packages, making it possible to write robotic software that can be reused on different hardware platforms. With thousands of packages available per stable distribution, encapsulating algorithms, sensor drivers, etc., it is the de facto middleware for robotics. Like any software ecosystem, ROS must evolve in order to keep meeting the requirements of its users. In practice, packages may end up being abandoned between releases: no one may be available to update a package, or newer packages offer similar functionality. As such, we wanted to identify and understand the evolution challenges faced by the ROS ecosystem. In this article, we report our findings after interviewing 19 ROS developers in depth, followed by a focus group (4 participants) and an online survey of 119 ROS community members. We specifically focused on the issues surrounding package reuse and how to contribute to existing packages. To conclude, we discuss the implications of our findings, and propose five recommendations for overcoming the identified issues, with the goal of improving the health of the ROS ecosystem.}
}
@article{DESPEAUX2007359,
title = {Abstracts},
journal = {Historia Mathematica},
volume = {34},
number = {3},
pages = {359-374},
year = {2007},
issn = {0315-0860},
doi = {https://doi.org/10.1016/j.hm.2007.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0315086007000353},
author = {Sloan Evans Despeaux and Laura Martini and Kim Plofker}
}
@article{KOLODNY2015252,
title = {The problem of multimodal concurrent serial order in behavior},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {56},
pages = {252-265},
year = {2015},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2015.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0149763415001943},
author = {Oren Kolodny and Shimon Edelman},
keywords = {Embodied cognition, Stimulus-response, Multimodality, Concurrency, Hierarchy, Language, Sensorimotor integration, Multidimensionality, Events, parallel computation, sequential behavior, Lashley, Serial order, Superior colliculus, Hippocampus, Basal ganglia, Unity of consciousness},
abstract = {The “problem of serial order in behavior,” as formulated and discussed by Lashley (1951), is arguably more pervasive and more profound both than originally stated and than currently appreciated. We spell out two complementary aspects of what we term the generalized problem of behavior: (i) multimodality, stemming from the disparate nature of the sensorimotor variables and processes that underlie behavior, and (ii) concurrency, which reflects the parallel unfolding in time of these processes and of their asynchronous interactions. We illustrate these on a number of examples, with a special focus on language, briefly survey the computational approaches to multimodal concurrency, offer some hypotheses regarding the manner in which brains address it, and discuss some of the broader implications of these as yet unresolved issues for cognitive science.}
}
@article{LAW2021100321,
title = {Augmented reality applications for K-12 education: A systematic review from the usability and user experience perspective},
journal = {International Journal of Child-Computer Interaction},
volume = {30},
pages = {100321},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100321},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000477},
author = {Effie Lai-Chong Law and Matthias Heintz},
keywords = {Augmented reality, Education, Usability, User experience, Systematic review},
abstract = {In the past two decades, we have witnessed soaring efforts in applying Augmented Reality (AR) technology in education. Several systematic literature reviews (SLRs) were conducted to study AR educational applications (AREAs) and associated methodologies, primarily from the pedagogical rather than from the human–computer interaction (HCI) perspective. These reviews vary in goal, scale, scope, technique, outcome and quality. To bridge the gaps identified in these SLRs, ours is to meet fourfold objectives: to ground the analysis deeper in the usability and user experience (UX) core concepts and methods; to study the learning effect and usability/UX of AREAs and their relations by learner age; to reflect on the prevailing SLR process and propose improvement; to draw implications for the future development of AREAs. Our searches in four databases returned 714 papers of which 42, together with 7 from three existing SLRs, were included in the final analysis. Several intriguing findings have been identified: (i) the insufficient grounding in usability/UX frameworks indicates that there seems a disconnection between the HCI and technology-enhanced learning community; (ii) a lack of innovative AR-specific usability/UX evaluation methods and the continuing reliance on questionnaire may hamper the advances of AREAs; (iii) the learner age seems not a significant factor in determining the perceived usability and UX or the learning effect of AREAs; (iv) a limited number of studies at home suggests the missed opportunity of mobilizing parents to support children to deploy AREAs in different settings; (v) the number of AREAs for children with special needs remains disappointedly low; (vi) the threat of predatory journals to the quality of bibliometric sources amplifies the need for a robust approach to the quality assessment for SLR and transparency of interim results. Implications of these issues for future research and practice on AREAs are drawn.}
}
@article{FIGL201596,
title = {Influence factors for local comprehensibility of process models},
journal = {International Journal of Human-Computer Studies},
volume = {82},
pages = {96-110},
year = {2015},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2015.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1071581915001019},
author = {Kathrin Figl and Ralf Laue},
keywords = {Deductive reasoning, Business process models, Model comprehension, Cognitive complexity},
abstract = {The main aim of this study is to investigate human understanding of process models and to develop an improved understanding of its relevant influence factors. Aided by assumptions from cognitive psychology, this article attempts to address specific deductive reasoning difficulties based on process models. The authors developed a research model to capture the influence of two effects on the cognitive difficulty of reasoning tasks: (i) the presence of different control-flow patterns (such as conditional or parallel execution) in a process model and (ii) the interactivity of model elements. Based on solutions to 61 different reasoning tasks by 155 modelers, the results from this study indicate that the presence of certain control-flow patterns influences the cognitive difficulty of reasoning tasks. In particular, sequence is relatively easy, while loops in a model proved difficult. Modelers with higher process modeling knowledge performed better and rated subjective difficulty of loops lower than modelers with lower process modeling knowledge. The findings additionally support the prediction that interactivity between model elements is positively related to the cognitive difficulty of reasoning. Our research contributes to both academic literature on the comprehension of process models and practitioner literature focusing on cognitive difficulties when using process models.}
}
@article{STEPHENS2021103466,
title = {Landscape changes and their hydrologic effects: Interactions and feedbacks across scales},
journal = {Earth-Science Reviews},
volume = {212},
pages = {103466},
year = {2021},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2020.103466},
url = {https://www.sciencedirect.com/science/article/pii/S0012825220305122},
author = {C.M. Stephens and U. Lall and F.M. Johnson and L.A. Marshall},
abstract = {Human activities have extensively altered landscapes throughout the world and further changes are expected in the future. Anthropogenic impacts such as land use change, groundwater extraction and dam construction, along with the effects of climate change, interact with natural factors including soil weathering and erosion. Together, these processes create a constantly shifting, dynamic terrestrial environment that violates the assumption of stationarity commonly applied in hydrology. Consequently, hydrologists need to rethink both statistical and calibrated models to account for complex environmental processes. We review the literature on human-landscape-hydrological interactions to identify processes and feedbacks that influence water balances. Most of the papers covered consider only a few of these processes at a time and focus on structural attributes of the interactions rather than the short and long-term dynamics. We identify challenges in representing the scale-dependence, environmental connectivity and human-water interactions that characterize complex, dynamic landscapes. A synthesis of the findings posits connections between different landscape changes, as well as the associated timescales and level of certainty. A case study explores how different processes could combine to drive long-term shifts in catchment behavior. Recognizing that some important questions remain unaddressed by traditional approaches, we suggest the concept of ‘big laboratories’ in which multifaceted experiments are conducted in the environment by artificially inducing landscape change. These experiments would be accompanied by mechanistic modeling to both untangle experimental results and improve the theoretical basis of environmental models. An ambitious program of physical and virtual experimentation is needed to progress hydrologic prediction for dynamic landscapes.
Plain language summary
The Earth’s surface is constantly changing due to human-driven and natural processes. Shifts may be driven by humans directly (e.g. via land use change) or indirectly (e.g. by driving climate change that causes shifts in ecological communities). Other changes are natural, such as certain soil processes that lead to shifts in texture and properties over time. In many places, landscape change is now occurring at unprecedented rates. This impacts the water cycle, creating a need for models that are robust under changing conditions. Our paper synthesizes a wide range of literature on key aspects of landscape change that have wide-ranging implications for hydrology. We focus on the impacts of processes at different spatial and temporal scales, along with feedbacks between various environmental and anthropogenic shifts. We discuss connections between different landscape changes and the timescales over which they each affect the water cycle. A case study is presented to highlight the potential for cascading landscape disturbances that could alter long-term catchment response. Recognizing limitations in traditional data collection and modeling, we introduce the concept of ‘big laboratories’ to conduct environmental experiments under landscape change, providing an avenue for addressing the complex questions around hydrology in a changing world.}
}
@article{OZTOP201343,
title = {Mirror neurons: Functions, mechanisms and models},
journal = {Neuroscience Letters},
volume = {540},
pages = {43-55},
year = {2013},
note = {The Mirror Neuron System},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2012.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0304394012013183},
author = {Erhan Oztop and Mitsuo Kawato and Michael A. Arbib},
keywords = {Mirror neuron, Computational model, Action recognition, imitation, language evolution, Mirror neuron development, Direct matching},
abstract = {Mirror neurons for manipulation fire both when the animal manipulates an object in a specific way and when it sees another animal (or the experimenter) perform an action that is more or less similar. Such neurons were originally found in macaque monkeys, in the ventral premotor cortex, area F5 and later also in the inferior parietal lobule. Recent neuroimaging data indicate that the adult human brain is endowed with a “mirror neuron system,” putatively containing mirror neurons and other neurons, for matching the observation and execution of actions. Mirror neurons may serve action recognition in monkeys as well as humans, whereas their putative role in imitation and language may be realized in human but not in monkey. This article shows the important role of computational models in providing sufficient and causal explanations for the observed phenomena involving mirror systems and the learning processes which form them, and underlines the need for additional circuitry to lift up the monkey mirror neuron circuit to sustain the posited cognitive functions attributed to the human mirror neuron system.}
}
@incollection{CARETTE202215,
title = {Chapter Two - Embracing the laws of physics: Three reversible models of computation},
editor = {Ali R. Hurson},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {126},
pages = {15-63},
year = {2022},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000838},
author = {Jacques Carette and Roshan P. James and Amr Sabry},
keywords = {Reversible programming, Reversible Boolean circuits, Monoidal categories, Type isomorphisms, Commutative semirings, Homotopy-type theory, Quantum circuits, Permutations},
abstract = {Our main models of computation (the Turing Machine and the RAM) and most modern computer architectures make fundamental assumptions about which primitive operations are realizable on a physical computing device. The consensus is that these primitive operations include logical operations like conjunction, disjunction and negation, as well as reading and writing to a large collection of memory locations. This perspective conforms to a macro-level view of physics and indeed these operations are realizable using macro-level devices involving thousands of electrons. This point of view is however incompatible with computation realized using quantum devices or analyzed using elementary thermodynamics as both these fundamental physical theories imply that information is a conserved quantity of physical processes and hence of primitive computational operations. Our aim is to redevelop foundational computational models in a way that embraces the principle of conservation of information. We first define what information is and what its conservation means in a computational setting. We emphasize the idea that computations must be reversible transformations on data. One can think of data as modeled using topological spaces and programs as modeled by reversible deformations of these spaces. We then illustrate this idea using three notions of data and their associated reversible computational models. The first instance only assumes unstructured finite data, i.e., discrete topological spaces. The corresponding notion of reversible computation is that of permutations. We show how this simple model subsumes conventional computations on finite sets. We then consider a modern structured notion of data based on the Curry–Howard correspondence between logic and type theory. We develop the corresponding notion of reversible deformations using a sound and complete programming language for witnessing type isomorphisms and proof terms for commutative semirings. We then “move up a level” to examine spaces that treat programs as data, which is a crucial notion for any universal model of computation. To derive the corresponding notion of reversible programs between programs, i.e., reversible program equivalences, we look at the “higher dimensional” analog to commutative semirings: symmetric rig groupoids. The coherence laws for these groupoids turn out to be exactly the sound and complete reversible program equivalences we seek. We conclude with some possible generalizations inspired by homotopy type theory and survey several open directions for further research.}
}
@incollection{WU2022293,
title = {Chapter 8 - Chronotopologic krigology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {293-344},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000034},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Geostatistical Kriging, Space-time ordinary, Simple, Indicator chronoblock and functional Kriging, Accuracy indicators, Cross-validation},
abstract = {Various types of chronotopologic Kriging are presented, including ordinary, simple and indicator Kriging of natural attribute distributions, as well as chronoblock and functional Kriging. Their specific properties, links to chronotopologic mapping and real-world application ranges are reviewed. Interpolation accuracy indicators and cross-validation tests are analyzed. The benefits and concerns of applied Krigology are outlined.}
}
@incollection{MOORE2024493,
title = {Chapter 17 - The application of knowledge in soil microbiology, ecology, and biochemistry (SMEB) to the solution of today’s and future societal needs},
editor = {Eldor A. Paul and Serita D. Frey},
booktitle = {Soil Microbiology, Ecology and Biochemistry (Fifth Edition)},
publisher = {Elsevier},
edition = {Fifth Edition},
pages = {493-536},
year = {2024},
isbn = {978-0-12-822941-5},
doi = {https://doi.org/10.1016/B978-0-12-822941-5.00017-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822941500017X},
author = {John C. Moore and Nathaniel Mueller},
keywords = {Biogeochemistry, climate change, science literacy, microbial communities, soil organic matter, food webs, sustainability development goals},
abstract = {This chapter presents an ecosystem-based framework for applying soil microbiology, ecology, and biochemistry (SMEB) principles and processes to address the preservation and sustainability of global soils to meet societal needs in the face of environmental change. The approach is organized around the UN Sustainable Development Goals, focusing on ecosystem services provided by soil microbes and soil biota at the nexus of food, water, and energy. The role of soil biota in ecosystem processes and the impacts of human activities on those processes are presented. The approach promotes conservation and regenerative methods that manage natural SMEB processes to optimize ecosystem services and minimize alterations in natural processes. Adoption and application of sound SMEB science will require a high degree of literacy among different stakeholders to codevelop management practices and regulatory policy. The approach also advocates promoting SMEB science literacy within the education system through reforms in science standards and curricula.}
}
@article{KHRENNIKOV2006225,
title = {Quantum-like brain: “Interference of minds”},
journal = {Biosystems},
volume = {84},
number = {3},
pages = {225-241},
year = {2006},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2005.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0303264705001942},
author = {Andrei Khrennikov},
abstract = {We present a contextualist statistical realistic model for quantum-like representations in physics, cognitive science, and psychology. We apply this model to describe cognitive experiments to check quantum-like structures of mental processes. The crucial role is played by interference of probabilities for mental observables. Recently one such experiment based on recognition of images was performed. This experiment confirmed our prediction on the quantum-like behavior of mind. In our approach “quantumness of mind” has no direct relation to the fact that the brain (as any physical body) is composed of quantum particles. We invented a new terminology “quantum-like (QL) mind.” Cognitive QL-behavior is characterized by a nonzero coefficient of interference λ. This coefficient can be found on the basis of statistical data. There are predicted not only cos⁡θ-interference of probabilities, but also hyperbolic cosh⁡θ-interference. This interference was never observed for physical systems, but we could not exclude this possibility for cognitive systems. We propose a model of brain functioning as a QL-computer (there is a discussion on the difference between quantum and QL computers).}
}
@article{REISENZEIN20096,
title = {Emotions as metarepresentational states of mind: Naturalizing the belief–desire theory of emotion},
journal = {Cognitive Systems Research},
volume = {10},
number = {1},
pages = {6-20},
year = {2009},
note = {Modeling the Cognitive Antecedents and Consequences of Emotion},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000272},
author = {Rainer Reisenzein},
keywords = {Emotion, Belief–desire theory, Metacognition, Affective computing, BDI},
abstract = {Describes the outlines of a computational explication of the belief–desire theory of emotion, a variant of cognitive emotion theory. According to the proposed explication, a core subset of emotions including surprise are nonconceptual products of hardwired mechanisms whose primary function is to subserve the monitoring and updating of the central representational system of humans, the belief–desire system. The posited emotion-producing mechanisms are analogous to sensory transducers; however, instead of sensing the world, they sense the state of the belief–desire system and signal important changes in this system, in particular the fulfillment and frustration of desires and the confirmation and disconfirmation of beliefs. Because emotions represent this information about the state of the representational system in a nonconceptual format, emotions are nonconceptual metarepresentations. It is argued that this theory of emotions provides for a deepened understanding of the role of emotions in cognitive systems and solves several problems of psychological emotion theory.}
}
@article{SERHOLT2018250,
title = {Breakdowns in children's interactions with a robotic tutor: A longitudinal study},
journal = {Computers in Human Behavior},
volume = {81},
pages = {250-264},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217307100},
author = {Sofia Serholt},
keywords = {Child–robot interaction, Education, Robotic tutor, Breakdowns, Interaction analysis, Thematic analysis},
abstract = {In recent years, there has been a growing research interest towards exploring the benefit of Child–Robot Interaction for educational purposes through the use of social robotics. Despite the label, such robots are typically only social within scripted activities. The current study takes a critical look at the case of a robotic tutor which was implemented in an elementary school for 3.5 months, where children repeatedly took turns interacting with the robot individually as well as in pairs. The aim of the study was to explore what caused breakdowns in children's interactions with the robotic tutor. In this qualitative study, over 14 h of video recordings of children's interaction sessions were analyzed in-depth through interaction analysis and thematic analysis. The results comprise four themes to explain why children's interactions with the robotic tutor break down: (1) the robot's inability to evoke initial engagement and identify misunderstandings, (2) confusing scaffolding, (3) lack of consistency and fairness, and finally, (4) controller problems. The implications of these breakdowns for the educational use of robots are discussed, and it is concluded that several challenges need to be rigorously addressed in order for robotic tutors to be able to feature in education.}
}
@article{HORVATH2015161,
title = {Ubiquitous computer aided design: A broken promise or a Sleeping Beauty?},
journal = {Computer-Aided Design},
volume = {59},
pages = {161-175},
year = {2015},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2014.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010448514002358},
author = {Imre Horváth and Regine W. Vroom},
keywords = {Ubiquitous computing, Computer aided design, Ubiquitous design enablers, Competing technology exploitation, Ubiquitous CAD applications},
abstract = {As a novel computational approach, ubiquitous computing was emerging at the beginning of the 1980s and has reached a rather mature level by now. It assumes that computing can be available anywhere, anytime and in any context due to technological developments, social demands and calm implementations. Over the years, the opportunities of this computing paradigm have been explored and the benefits have been exploited successfully in many application fields. This survey paper addresses ubiquitous computing from the perspective of enabling computer aided design. The specific objectives of the reported survey are to: (i) give an overall account of the current status of ubiquitous computing and technologies, (ii) cast light on how ubiquitous computing has influenced the development of CAD systems, tools, and methods, and (iii) critically investigate future development opportunities of ubiquitous computing enabled computer aided design. First, the paper discusses the principles and typical technologies of ubiquitous computing. Then, the development and spectrum of the so-called standard computer aided design tasks are analyzed from a computational point of view. Afterwards, the already implemented design enabling functionalities are discussed and some additional functional possibilities are considered. The literature provides evidence that ubiquitous computing has not managed to revolutionize the methodologies or the systems of computer aided design so far, though many researchers intensively studied the affordances and the application possibilities of ubiquitous technologies. One reason is that ubiquitous computing technologies had in the last two decades to compete with other kinds of computational technologies, such as high-capacity computing, high-speed networking, immersive virtual reality, knowledge ontologies, smart software agents, mobile communication, etc., which had a much stronger influence on the development of computer aided design methods and systems. In combination with the rather conservative and conventionalist industrial practice of CAD system development and application, this may explain why the ubiquitous computing revolution remained weak in computer aided design. The literature clearly indicates that application of ubiquitous technologies did not lead to radically new functionalities that could have been exploited by the concerned industries. Consequently, it seems to be possible that computer aided design simply steps over the paradigm of ubiquitous computing and expects new functionalities from the emerging new computing paradigms, such as brain–computer interfacing, cyber–physical computing, biological computing, or quantum computing.}
}
@article{BARRON2010178,
title = {Predictors of creative computing participation and profiles of experience in two Silicon Valley middle schools},
journal = {Computers & Education},
volume = {54},
number = {1},
pages = {178-189},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2009.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S036013150900195X},
author = {Brigid Barron and Sarah E. Walter and Caitlin Kennedy Martin and Colin Schatz},
keywords = {Digital divide, Learning ecology, Home access},
abstract = {Examination of the “digital divide” has increasingly gone beyond the study of differences in physical access to computers to focus on individuals’ use of technological tools for empowered and generative uses. In this research study, we investigated the relationship between access to tools and experience with creative production activities. Our participants included 160 8th grade learners from two public middle schools. The local communities represented by the two schools differed in parent education levels, proportion of recent immigrants, and average family income. Findings indicated substantial variability in students’ history of creative production experiences within both communities. Three sets of analyses were completed. First, the two school populations were compared with respect to average levels of student creative production experience, access to tools at home, use of learning resources, frequency of technology use, and access to computing outside of their home. Second, correlates of variability in individuals’ breadth of experience with creative production activities were explored across both schools through a regression analysis. The resulting model indicated that students’ experience was best predicted by the number of technology tools available at home, number of learning resources used, frequency of computer use at home, and non-home access network size. In a third analysis, profiles of experience were created based on both breadth and depth of experience; the resulting four groups of students were compared. More experienced students utilized a broader range of learning resources, had access to more tools at home, taught a wider range of people, and were more confident in their computing skills. The groups did not differ in their self-reports of interest in learning more about technology.}
}
@incollection{COOPER201313,
title = {On Computable Numbers, with an Application to the Entscheidungsproblem – A Correction},
editor = {S. Barry Cooper and Jan Van Leeuwen},
booktitle = {Alan Turing: His Work and Impact},
publisher = {Elsevier},
address = {Boston},
pages = {13-115},
year = {2013},
isbn = {978-0-12-386980-7},
doi = {https://doi.org/10.1016/B978-0-12-386980-7.50002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123869807500022},
author = {S. Barry Cooper and Jan Van Leeuwen}
}
@incollection{WILLIAMS202351,
title = {Part Two—The international benchmarking exercise},
editor = {David Baker and Lucy Ellis and Caroline Williams and Cliff Wragg},
booktitle = {Benchmarking Library, Information and Education Services},
publisher = {Chandos Publishing},
pages = {51-107},
year = {2023},
isbn = {978-0-323-95662-8},
doi = {https://doi.org/10.1016/B978-0-323-95662-8.00015-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956628000151},
author = {Caroline Williams and Cliff Wragg}
}
@incollection{KIRK201771,
title = {Chapter 4 - Memory and data locality},
editor = {David B. Kirk and Wen-mei W. Hwu},
booktitle = {Programming Massively Parallel Processors (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
pages = {71-101},
year = {2017},
isbn = {978-0-12-811986-0},
doi = {https://doi.org/10.1016/B978-0-12-811986-0.00004-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119860000042},
author = {David B. Kirk and Wen-mei W. Hwu},
keywords = {Memory bandwidth, memory-bound, on-chip memory, tiling, strip-mining, shared memory, private memory, scope, lifetime, occupancy},
abstract = {This chapter introduces the concepts of memory bound application. It uses matrix multiplication to illustrate opportunities for reducing the number of global memory accesses. It then introduces the tiling technique where barrier synchronization is used to coordinate the timing of executing threads for improved locality and reduced global memory accesses. The tiling techniques, however, involve additional complexities in boundary checks. The chapter uses matrix multiplication to illustrate the additional boundary checks needed for a tiled kernel to be applicable to arbitrary matrix sizes. The chapter concludes with an overview of how usage of shared memory and registers can affect the number of thread blocks that can be accommodated in each Streaming Multiprocessor.}
}
@article{KHALAJZADEH2025107570,
title = {Accessibility of low-code approaches: A systematic literature review},
journal = {Information and Software Technology},
volume = {177},
pages = {107570},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107570},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001757},
author = {Hourieh Khalajzadeh and John Grundy},
keywords = {Systematic literature review, Low-code, Visual languages, Block-based programming, Accessibility},
abstract = {Context:
Model-driven approaches are increasingly used in different domains, such as education, finance and app development, in order to involve non-developers in the software development process. Such tools are hugely dependent on visual elements and thus might not be accessible for users with specific challenges, e.g., visual impairments.
Objectives:
To locate and analyse existing literature on the accessibility of low-code approaches, their strengths and weaknesses and key directions for future research.
Methods:
We carried out a systematic literature review and searched through five leading databases for primary studies. We used both quantitative and qualitative methods for data synthesis.
Results:
After reviewing and filtering 918 located studies, and conducting both backward and forward snowballing, we identified 38 primary studies that were included in our analysis. We found most papers focusing on accessibility of visual languages and block-based programming.
Conclusion:
Limited work has been done on improving low code programming environment accessibility. The findings of this systematic literature review will assist researchers and developers in understanding the accessibility issues in low-code approaches and what has been done so far to develop accessible approaches.}
}
@article{MELVILLE202198,
title = {Abstracts},
journal = {Historia Mathematica},
volume = {55},
pages = {98-114},
year = {2021},
issn = {0315-0860},
doi = {https://doi.org/10.1016/j.hm.2021.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0315086021000021},
author = {Duncan J. Melville and Laura Martini and Kim Plofker}
}
@article{SMITH1991251,
title = {The owl and the electric encyclopedia},
journal = {Artificial Intelligence},
volume = {47},
number = {1},
pages = {251-288},
year = {1991},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(91)90056-P},
url = {https://www.sciencedirect.com/science/article/pii/000437029190056P},
author = {Brian Cantwell Smith},
abstract = {A review of “On the thresholds of knowledge”, by D.B. Lenat and E.A. Feigenbaum.}
}
@article{LEECULTURA2022100355,
title = {Children’s play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach},
journal = {International Journal of Child-Computer Interaction},
volume = {31},
pages = {100355},
year = {2022},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100355},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000647},
author = {Serena Lee-Cultura and Kshitij Sharma and Michail Giannakos},
keywords = {Multimodal data, MMLA, Learning Analytics, Sensors, Cognitive Load Theory, Educational Technologies, Embodied Interaction, Multi-Modal mixed methods approach},
abstract = {Motion-Based Learning Technologies (MBLT) offer a promising approach for integrating play and problem-solving behaviour within children’s learning. The proliferation of sensor technology has driven the field of learning technology towards the development of tools and methods that may benefit from the produced Multi-Modal Data (MMD). Such data can be used to uncover cognitive, affective and physiological processes during learning activities. Combining MMD with more traditionally exercised assessment tools, such as video content analysis, provides a more holistic understanding of children’s learning experiences and has the potential to enable the design of educational technologies capable of harmonising children’s cognitive, affective and physiological processes, while promoting appropriately balanced play and problem-solving efforts. However, the use of an MMD mixed methods approach that combines qualitative and MMD data to understand children’s behaviours during engagement with MBLT is rather unexplored. We present an in-situ study where 26 children, ages 10–12, solved a motion-based sorting task for learning geometry. We continuously and unobtrusively monitored children’s learning experiences using MMD collection via eye-trackers, wristbands, Kinect joint tracking, and a web camera. We devised SP3, a novel observational scheme that can be used to understand children’s solo interactions with MBLT, and applied it to identify and extract children’s evoked play and problem-solving behaviour. Collective analysis of the MMD and video codes provided explanations of children’s task performance through consideration of their holistic learning experience. Lastly, we applied predictive modelling to identify the synergies between various MMD measurements and children’s play and problem-solving behaviours. This research sheds light on the opportunities offered in the confluence of video coding (a traditional method in learning sciences) and MMD (an emerging method that leverages sensors proliferation) for investigating children’s behaviour with MBLT.}
}
@incollection{WANG2021157,
title = {Chapter 5 - Battery state-of-charge estimation methods},
editor = {Shunli Wang and Yongcun Fan and Daniel-Ioan Stroe and Carlos Fernandez and Chunmei Yu and Wen Cao and Zonghai Chen},
booktitle = {Battery System Modeling},
publisher = {Elsevier},
pages = {157-198},
year = {2021},
isbn = {978-0-323-90472-8},
doi = {https://doi.org/10.1016/B978-0-323-90472-8.00009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323904728000093},
author = {Shunli Wang and Yongcun Fan and Daniel-Ioan Stroe and Carlos Fernandez and Chunmei Yu and Wen Cao and Zonghai Chen},
keywords = {State-of-charge estimation, Coordinate transformation, Binary iterative algorithm, Extended Kalman filtering, Equivalent modeling, Correction strategy, Thermal influencing effect, Time-varying current condition, Complex current rate verification},
abstract = {With the large-scale promotion of new energy vehicles, the demand for power batteries such as the lithium-ion type has increased. Its service lifespan is closely related to the service condition. The important embodiment of the service condition is the battery state of charge, which can reflect its residual capacity. Through accurate residual capacity, the battery application strategy can be planned to realize the battery operation of the best condition. The long service lifespan can be realized by adjusting the voltage and current. Therefore, real-time accurate state estimation has a significant effect on battery management. The extended Kalman filtering algorithm is introduced to estimate the state value under complex working conditions. The overview of the state-of-charge estimation is first conducted. After that, the equivalent modeling construction method is explored. The coordinate transformation treatment is implemented in the binary iterative calculation algorithm, according to which the algorithm is implemented. In the extended calculation process, the iterative prediction and correction strategies are introduced into the test. Consequently, the pulse-power characteristic test is conducted to obtain the estimation features, considering a thermal influencing effect, a time-varying current condition, and a complex current rate verification.}
}
@incollection{PAYDAR2023165,
title = {Chapter 3 - Nuclear power reactions driven radiation hardening environments},
editor = {Ali Zamani Paydar and Seyed Kamal Mousavi Balgehshiri and Bahman Zohuri},
booktitle = {Advanced Reactor Concepts (ARC)},
publisher = {Elsevier},
pages = {165-233},
year = {2023},
isbn = {978-0-443-18989-0},
doi = {https://doi.org/10.1016/B978-0-443-18989-0.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044318989000003X},
author = {Ali Zamani Paydar and Seyed Kamal Mousavi Balgehshiri and Bahman Zohuri},
keywords = {Nuclear power plants, Design basis accident, Loss of coolant accident, Shielding, Neutron reflector, Nuclear submarines, Halving thickness values, Artificial intelligence},
abstract = {Radiation hardening, also known as “rad hardening,” and radiation survivability testing are of critical importance to defense, aerospace, and energy industries. Everyone knows that excessive exposure to radiation can cause severe damage to living things, but high radiation levels can also cause radiation damage to other objects, especially electronics. Ionizing radiation in particular, including directly ionizing radiation such as alpha and beta particles and indirectly ionizing radiation such as gamma rays and neutron radiation, is profoundly damaging to the semiconductors that make up the backbone of all modern electronics. Just one charged particle can interfere with thousands of electrons, causing signal noise, disrupting digital circuits, and even causing permanent physical radiation damage. Radiation hardening involves designing radiation-tolerant electronics and components that are tolerant of the massive levels of ionizing radiation, such as cosmic outer space radiation, X-ray radiation in medical or security environments, and high energy radiation within nuclear power plants. In order to test these components and determine whether they are sufficiently hardened, radiation-hardened electronics manufacturers perform rigorous testing as part of their product manufacturing processes. Components which pass these tests go into production and can be described as “radiation-hardened”; components that do not go back to design.}
}
@article{BARFAR2021113442,
title = {Peak cubes in service operations: Bringing multidimensionality into decision support systems},
journal = {Decision Support Systems},
volume = {140},
pages = {113442},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113442},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301974},
author = {Arash Barfar and Balaji Padmanabhan and Alan Hevner},
keywords = {Multidimensional decision support, Peak-end rule, Peak cubes, Pareto frontiers, Skylines, Shapley values},
abstract = {Companies like Ritz Carlton, Disney and Verizon are among many who have invested in analytics to improve their customers' service experiences with the firms. Extensive data are collected on all aspects of how customers interact or experience the products or services. Research has shown the importance of the “peak-end” rule in service design; that is, providing a customer with good “peak” service levels and “ending” the service experience with high quality can enhance customer satisfaction and build loyalty. However, previous studies have examined this phenomenon only in contexts with unidimensional service levels. We introduce peak cubes, which enable service designers and scholars to pinpoint prominent service levels in multidimensional service experience profiles—thereby extending current research on behavioral economics and service design to more general settings. Results indicate the potential of multidimensional peak-end models to better predict customer satisfaction in various service scenarios. Using Shapley values in coalitional game theory, the resulting models can also inform service designers about the quality dimensions that are critical from the perspective of multidimensional peak-end heuristic and customer satisfaction. Our research contributions and proposed methodology will enhance decision support systems with multidimensional capabilities and have applications to fields as diverse as service operations and healthcare.}
}
@article{MEYER202013,
title = {Changing Design Education for the 21st Century},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {6},
number = {1},
pages = {13-49},
year = {2020},
note = {Design Education. Part I},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2019.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872620300046},
author = {Michael W. Meyer and Don Norman},
keywords = {Design education, Design-driven transformation, Design thinking, Design doing, Major societal challenges, Complex sociotechnical systems, DesignX},
abstract = {Designers are entrusted with increasingly complex and impactful challenges. However, the current system of design education does not always prepare students for these challenges. When we examine what and how our system teaches young designers, we discover that the most valuable elements of the designer’s perspective and process are seldom taught. Instead, some designers grow beyond their education through their experience working in industry, essentially learning by accident. Many design programs still maintain an insular perspective and an inefficient mechanism of tacit knowledge transfer. Meanwhile, skills for developing creative solutions to complex problems are increasingly essential. Organizations are starting to recognize that designers bring something special to this type of work, a rational belief based upon numerous studies that link commercial success to a design-driven approach. So, what are we to do? Other learned professions such as medicine, law, and business provide excellent advice and guidance embedded within their own histories of professionalization. In this article, we borrow from their experiences to recommend a course of action for design. It will not be easy: it will require a study group to make recommendations for a roster of design and educational practices that schools can use to build a curriculum that matches their goals and abilities. And then it will require a conscious effort to bootstrap the design profession toward both a robust practitioner community and an effective professoriate, capable together of fully realizing the value of design in the 21st century. In this article, we lay out that path.}
}
@article{FRIENDSOFAJPM2013687,
title = {Notes from the Field: Planting, Nurturing, and Watching Things Grow},
journal = {American Journal of Preventive Medicine},
volume = {45},
number = {6},
pages = {687-702},
year = {2013},
issn = {0749-3797},
doi = {https://doi.org/10.1016/j.amepre.2013.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S074937971300500X},
author = { {Friends of AJPM}}
}
@incollection{HAMILTON2023371,
title = {Chapter 16 - Natural Language Processing},
editor = {Julien Delarue and J. Ben Lawlor},
booktitle = {Rapid Sensory Profiling Techniques (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {371-410},
year = {2023},
series = {Woodhead Publishing Series in Food Science, Technology and Nutrition},
isbn = {978-0-12-821936-2},
doi = {https://doi.org/10.1016/B978-0-12-821936-2.00004-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219362000042},
author = {Leah Marie Hamilton and Jacob Lahne},
keywords = {Natural Language Processing, Machine learning, Deep learning, Text analysis, Computational linguistics, Sensory evaluation, Descriptive analysis},
abstract = {Sensory evaluation is predicated on the use and interpretation of human language. We ask our subjects to describe their sensory experiences and affective responses, which we cannot directly observe. This formulation of sensory science encourages direct engagement with linguistics and in particular, a recent subfield of linguistics, computer science, and artificial intelligence called “Natural Language Processing” (NLP, sometimes “computational linguistics”). In this chapter we will provide an introduction to Natural Language Processing (NLP) for sensory scientists who wish to employ NLP as a rapid method for sensory evaluation. Because NLP is a large, diverse, and rapidly evolving field, we will begin with a brief, pragmatic overview of the discipline, with an emphasis on key historical and current methods and applications. We will then briefly discuss the linguistic perspective and its application to sensory evaluation, with an aim to motivating the remaining chapter. Following that, we will discuss key areas of NLP, from data collection to processing to analysis to advanced applications. Throughout the chapter, we will use a consistent case study of natural-language descriptions for a food product to provide examples and illustrate NLP methods.}
}
@incollection{DOMINOWSKI19941,
title = {CHAPTER 1 - History of Research on Thinking and Problem Solving},
editor = {Robert J. Sternberg},
booktitle = {Thinking and Problem Solving},
publisher = {Academic Press},
address = {San Diego},
pages = {1-35},
year = {1994},
volume = {2},
series = {Handbook of Perception and Cognition},
isbn = {978-0-08-057299-4},
doi = {https://doi.org/10.1016/B978-0-08-057299-4.50007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080572994500074},
author = {Roger L. Dominowski and Lyle E. Bourne},
abstract = {Publisher Summary
The two problems that characterize the modern psychology of thinking are mental representation and mental computation. Throughout the history of psychology, there has been an agreement that the essential features of a problem are that an organism has a goal but lacks a clear or well-learned route to the goal. Thus the emphasis in research on problem solving has been on response discovery—how the organism arrives at an effective, goal-attaining behavior. There have often been controversies over the role of learning or past experiences in problem solving. This chapter illustrates the conflict between emphases on learning and emphases on perception as central components of problem solving. Because a problem solver must find a solution, it might seem inevitable that an essential activity tries different approaches makes errors until the right approach is found. Earlier in problem-solving research, trial and error became associated with the view that acquiring a solution was a gradual, undirected process that did not involve perception or comprehension of problem requirements or structure.}
}
@incollection{2023861,
title = {Index},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {861-952},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.18001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305180017}
}
@article{201943,
title = {Literature Listing},
journal = {World Patent Information},
volume = {56},
pages = {43-59},
year = {2019},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S017221901830156X}
}
@article{SALAJ2024298,
title = {Competencies for Smart City Challenges},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {3},
pages = {298-303},
year = {2024},
note = {22nd IFAC Conference on Technology, Culture and International Stability TECIS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.167},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324002507},
author = {Alenka Temeljotov Salaj and Olav Torp and Elham Andalib},
keywords = {smart solutions, competencies, AI, education},
abstract = {It is acknowledged that technological innovation is needed in all sectors to cope with new demands. From social innovations, it introduces novel ideas, whether products, services, or models, to fulfil societal needs and foster new partnerships or collaborations. The aim is to enhance social interactions and elevate human well-being. Development of cutting-edge digital technologies is reality. The challenge is on human resource side, how quickly we can prepare employees to adapt to the requirements of Industry 4.0 and Society 5.0. In the paper, the new competencies were identified with the business stakeholders by conducting a survey among industry partners to recognize the requirements for the future labor market. The stakeholders in the construction field act as target groups for monitoring and development of the competencies. The focus of the result part is on the competencies companies mostly miss from their employees from digital perspectives, e.g. reason to hire highly educated people, training possibilities for digitally upskilling employees, lacking appropriate competencies (critical thinking, systems and analytical thinking, information management, advanced computer/IT skills (AI), ensuring security). Digital skills Advanced data/IT skills were the competencies in that companies defined as key competencies expected to be developed in 21st-century higher education employees. It is highly important to consider the job market needs for development and to adopt the engineering education system to be responsive to the needs of labor market.}
}
@article{WOOD1990317,
title = {Verification of rule-based expert systems},
journal = {Expert Systems with Applications},
volume = {1},
number = {3},
pages = {317-322},
year = {1990},
note = {Special Issue: Verification and Validation of Knowledge-Based Systems},
issn = {0957-4174},
doi = {https://doi.org/10.1016/0957-4174(90)90010-R},
url = {https://www.sciencedirect.com/science/article/pii/095741749090010R},
author = {William T. Wood and Elaine N. Frankowski},
abstract = {We are investigating the problem of establishing computational rather than syntactic properties of forward-chaining rule-based expert systems. We model an expert system as a computation on working memory, define its execution semantics, and present proof techniques suitable for those semantics. Specifically, we model execution as a Dijkstra guarded-do construct, and use Dijkstra's Invariance Theorem and weakest precondition predicate transformers to establish invariants (safety properties) and postconditions (liveness properties). Our approach is an application of well-developed methods developed by Dijkstra and others for the verification of procedural programs. This paper introduces the approach, reports some initial results, and discusses future work.}
}
@article{TAN2020101793,
title = {A graph-theoretic approach for the detection of phishing webpages},
journal = {Computers & Security},
volume = {95},
pages = {101793},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101793},
url = {https://www.sciencedirect.com/science/article/pii/S016740482030078X},
author = {Choon Lin Tan and Kang Leng Chiew and Kelvin S.C. Yong and San Nah Sze and Johari Abdullah and Yakub Sebastian},
keywords = {Phishing detection, Hyperlinks, Web graph, Graph features, Machine learning},
abstract = {Over the years, various technical means have been developed to protect Internet users from phishing attacks. To enrich the anti-phishing efforts, we capitalise on concepts from graph theories, and propose a set of novel graph features to improve the phishing detection accuracy. The initial phase of the proposed technique involved the extraction of hyperlinks in the webpage under scrutiny and fetching the corresponding neighbourhood webpages. During this process, the page linking data were collected, and used to construct a web graph which models the overall hyperlink and network structure of the webpage. From the web graph, graph measures were computed and extracted as graph features to derive a classifier for detecting phishing webpages. Experimental results show that the proposed graph features achieve an improved overall accuracy of 97.8% when C4.5 was utilised as classifier, outperforming the existing conventional features derived from the same data samples. Unlike conventional features, the proposed graph features leverage inherent phishing patterns that are only visible at a higher level of abstraction, thus making it robust and difficult to be evaded by direct manipulations on the webpage contents. Our proposed graph-based technique also shows promising results when benchmarked against a prominent phishing detection technique. Hence, the proposed technique is an important contribution to the existing anti-phishing research towards improving the detection performance.}
}
@incollection{KOLTAY20161,
title = {Chapter 1 - Shifting Research Paradigms Toward Research 2.0},
editor = {Tibor Koltay and Sonja Špiranec and László Z. Karvalics},
booktitle = {Research 2.0 and the Future of Information Literacy},
publisher = {Chandos Publishing},
pages = {1-59},
year = {2016},
isbn = {978-0-08-100075-5},
doi = {https://doi.org/10.1016/B978-0-08-100075-5.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081000755000018},
author = {Tibor Koltay and Sonja Špiranec and László Z. Karvalics},
keywords = {Research paradigms, Research 2.0, Researchers’ skills and abilities, Open science, Open access, The data-intensive paradigm of scientific research, Alternative metrics of scientific output},
abstract = {This chapter discusses how Research 2.0 came into existence and how it developed into a leading paradigm of our era. This requires an outline of the socio-technical changes brought about by the development and widespread use of information and communications technologies, based on computers and leading to the appearance of social media. There is no one who would deny that researchers are central figures in research, so their skills and abilities will be briefly examined. Research 2.0 is closely connected to the idea of open science that will be described, giving especial attention to its main constituent that is open access. Open science also comprises the data-intensive paradigm of scientific research, which we consider in detail. A wider uptake of Research 2.0 is inhibited by a number of the factors of scholarly communication, so we will enumerate them.}
}
@article{LI2022100980,
title = {RETRACTED: Studying creativity and critical thinking skills at university and students' future income},
journal = {Thinking Skills and Creativity},
volume = {43},
pages = {100980},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2021.100980},
url = {https://www.sciencedirect.com/science/article/pii/S1871187121001954},
author = {Weijuan Li},
abstract = {This article has been retracted: please see Elsevier Policy on Article Withdrawal (https://www.elsevier.com/about/our-business/policies/article-withdrawal). This article has been retracted at the request of the Editors-in-Chief. After a thorough investigation, the Editors have concluded that the acceptance of this article was partly based upon the positive advice of one illegitimate reviewer report. The report was submitted from an email account which was provided to the journal as a suggested reviewer during the submission of the article. Although purportedly a real reviewer account, the Editors have concluded that this was not of an appropriate, independent reviewer. This manipulation of the peer-review process represents a clear violation of the fundamentals of peer review, our publishing policies, and publishing ethics standards. Apologies are offered to the reviewer whose identity was assumed and to the readers of the journal that this deception was not detected during the submission process.}
}
@article{ROBSON2022101018,
title = {Searching for the principles of a less artificial A.I.},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101018},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101018},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001617},
author = {B. Robson and G. Ochoa-Vargas},
keywords = {AI, Algorithms, X factor, Emergent properties, Consciousness, Quantum effects},
abstract = {What would it take to build a computer physician that can take its place amongst human peers? Currently, Neural Nets, especially as so-called “Deep Learning” nets, dominate what is popularly called “Artificial Intelligence”, but to many critics they seem to be little more than powerful data-analytic tools inspired by some of the more basic functions and regions of the human brain such as those involved in early processes in biological vision, classification, and categorization. The deeper nature of human intelligence as the term is normally meant, including relating to consciousness, has been the domain of philosophers, psychologists, and some neuroscientists. Now, attention is turning to neuronal mechanisms in humans and simpler organisms as a basis of a truer AI with far greater potential. Arguably, the approach required should be rooted in information theory and algorithmic science. But as discussed in this paper, caution is required: “just any old information” might not do. The information might need to be of a particular dynamical and actioning nature, and that might significantly impact the kind of computation and computer hardware required. Overall, however, the authors do not favor emergent properties such as those based on complexity and quantum effects. Despite the possible difficulties, such studies could, in return, have substantial benefits for biology and medicine beyond the computational tools that they produce to serve those disciplines.}
}
@article{ROMEROORGANVIDEZ2024112029,
title = {Data visualization guidance using a software product line approach},
journal = {Journal of Systems and Software},
volume = {213},
pages = {112029},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112029},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000724},
author = {David Romero-Organvidez and Jose-Miguel Horcas and José A. Galindo and David Benavides},
keywords = {Effective communication, Graphs, Tables, Software product line, Variability, Visualization},
abstract = {Data visualization aims to convey quantitative and qualitative information effectively by determining which techniques and visualizations are most appropriate for different situations and why. Various software solutions can produce numerous visualizations of the same data set. However, data visualization encompasses a wide range of visual configurations that depend on factors such as the type of data being displayed, the different displays (e.g., scatter plots, line graphs, and pie charts), the visual components used to represent the data (e.g., lines, dots, and bars), and the specific visual attributes of those components (e.g., color, shape, size, and length). A similar problem arises when designing data tables, where the dimensionality of the data and its complexity influence the choice of the most appropriate structure (e.g., unidirectional, bidirectional). Often, this broad spectrum of configurations requires a visualization expert who knows which techniques are best for which type of data source and what is to be conveyed. Typically, researchers and developers lack knowledge of data visualization best practices and must learn the design principles that enable effective communication and the technical details of the specific software tool they use to generate visualizations. This paper proposes a software product line approach to model and realize the variability of the visualization design process, using feature models to encode knowledge about design best practices in graphs and charts. Our approach involves solving visualization design variability through a stepwise configuration process and evaluating the proposal for a specific software visualization tool. Our solution facilitates effective communication of quantitative results by helping researchers and developers select and generate the most effective visualizations for each case. This approach opens up new opportunities for research at the intersection of data visualization and variability.}
}
@article{BARRICELLI2019101,
title = {End-user development, end-user programming and end-user software engineering: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {149},
pages = {101-137},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.11.041},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302577},
author = {Barbara Rita Barricelli and Fabio Cassano and Daniela Fogli and Antonio Piccinno},
keywords = {Systematic mapping study, End-user development, End-user programming, End-user software engineering},
abstract = {End-User Development (EUD), End-Programming (EUP) and End-User Software Engineering (EUSE) are three related research fields that study methods and techniques for empowering end users to modify and create digital artifacts. This paper presents a systematic mapping study aimed at identifying and classifying scientific literature about EUD, EUP and EUSE in the time range January 2000–May 2017. We selected 165 papers found through a manual selection of papers from specific conferences, journal special issues, and books, integrated with an automatic search on the most important digital libraries. The answer to our research question was built through a classification of the selected papers on seven dimensions: type of approach, interaction technique, phase in which the approach is adopted, application domain, target use, class of users, and type of evaluation. Our findings suggest that EUD, EUP and EUSE are active research topics not only in Human–Computer Interaction, but also in other research communities. However, little cross-fertilization exists among the three themes, as well as unifying frameworks and approaches for guiding novice designers and practitioners. Other findings highlight trends and gaps related to the analysis’ dimensions, which have implications on the design of future tools and suggest open issues for further investigations.}
}
@article{ALDAAJEH2022102754,
title = {The role of national cybersecurity strategies on the improvement of cybersecurity education},
journal = {Computers & Security},
volume = {119},
pages = {102754},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102754},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822001493},
author = {Saleh AlDaajeh and Heba Saleous and Saed Alrabaee and Ezedin Barka and Frank Breitinger and Kim-Kwang {Raymond Choo}},
keywords = {Cybersecurity strategic plan, Cybersecurity education, NICE framework, Cybersecurity curricula, GQO+Strategies paradigm},
abstract = {Digital information and telecommunication technologies have not only become essential to individuals’ daily lives but also to a nation’s sustained economic growth, societal well-being, critical infrastructure resilience, and national security. Consequently, the protection of a nation’s cyber sovereignty from malicious acts is a major concern. This signifies the importance of cybersecurity education in facilitating the creation of a resilient cybersecurity ecosystem and in supporting cyber sovereignty. This study reviews a sample from world-leading countries National Cybersecurity Strategic Plans (NCSPs) and analyzes the associated existing cybersecurity education and training improvement initiatives. Furthermore, a proposal to adopt the Goal-Question-Outcomes(GQO)+Strategies paradigm into cybersecurity education and training programs curricula improvement to national cybersecurity strategic goals is presented. The proposal maps cybersecurity strategic goals to cybersecurity skills and competencies using the National Initiative for Cybersecurity Education (NICE) framework. The newly proposed cybersecurity education and training programs’ curricula learning outcomes were generated from the GQO+Strategies paradigm based on the three major cybersecurity strategic goals: Development of secure digital and information technology infrastructure and services, defending from sophisticated cyber threats, and enrichment of individuals’ cybersecurity maturity and awareness. It is highly recommended that cybersecurity university program administrators utilize the proposed GQO+Strategies to align their program’s curriculum to NCSP. Hence, closing the gap that exists with the relevant skills and sustain national cybersecurity workforces.}
}
@article{BRINSON2015218,
title = {Learning outcome achievement in non-traditional (virtual and remote) versus traditional (hands-on) laboratories: A review of the empirical research},
journal = {Computers & Education},
volume = {87},
pages = {218-237},
year = {2015},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2015.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0360131515300087},
author = {James R. Brinson},
keywords = {Distance education and telelearning, Distributed learning environments, Evaluation of CAL systems, Simulations, Teaching/learning strategies},
abstract = {This review presents the first attempt to synthesize recent (post-2005) empirical studies that focus on directly comparing learning outcome achievement using traditional lab (TL; hands-on) and non-traditional lab (NTL; virtual and remote) participants as experimental groups. Findings suggest that most studies reviewed (n = 50, 89%) demonstrate student learning outcome achievement is equal or higher in NTL versus TL across all learning outcome categories (knowledge and understanding, inquiry skills, practical skills, perception, analytical skills, and social and scientific communication), though the majority of studies (n = 53, 95%) focused on outcomes related to content knowledge, with most studies (n = 40, 71%) employing quizzes and tests as the assessment instrument. Scientific inquiry skills was the least assessed learning objective (n = 4, 7%), and lab reports/written assignments (n = 5, 9%) and practical exams (n = 5, 9%) were the least common assessment instrument. The results of this review raise several important concerns and questions to be addressed by future research.}
}
@article{MOHDAMINUDDIN2023103582,
title = {The rise of website fingerprinting on Tor: Analysis on techniques and assumptions},
journal = {Journal of Network and Computer Applications},
volume = {212},
pages = {103582},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103582},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523000012},
author = {Mohamad Amar Irsyad {Mohd Aminuddin} and Zarul Fitri Zaaba and Azman Samsudin and Faiz Zaki and Nor Badrul Anuar},
keywords = {Security, Privacy, Anonymity, Tor, Traffic analysis, Website fingerprinting},
abstract = {Tor is one of the most popular anonymity networks that allows Internet users to hide their browsing activity. Hiding browsing activity is essential for Internet users to increase their privacy. Only Tor users should know the website they are browsing. However, an attacker can utilise the Website Fingerprinting (WF) attack to identify webpages browsed by Tor users. WF is a significant threat to Internet users' privacy as Tor should conceal the browsed webpages' information. Existing WF studies focused on the investigation to improve the identification capabilities, overlooking the systematic discussion and assessment of existing techniques. In addition, existing surveys and analyses reviewed insufficient variation of WF on Tor techniques. Therefore, this survey paper aims to provide a systematic and thorough review of various WF on Tor techniques. First, we discuss WF on Tor techniques in five primary aspects: threat model, victim target, website realm, traffic feature, and traffic classifier. We analyse and classify the reviewed studies on each WF aspect. The classification facilitates in-depth understanding and comparison between WF on Tor techniques. Furthermore, this paper investigates nine assumptions exercised in WF on Tor: closed-world, sequential browsing, isolated traffic, replicability, traffic parsing, passive webpage, disabled cache, static content, and single webpage. These assumptions limit the WF on Tor's practicality in real-world scenarios. Our analysis and classification indicate that most WF on Tor studies apply these assumptions despite being only suitable in controlled environments or laboratory experiments. In addition, most reviewed studies often lack transparency on the assumptions exercised in their studies, risking misunderstanding the WF on Tor techniques' actual practicality. At the end of this survey, we present WF on Tor taxonomy and highlight 21 WF on Tor research's limitations and gaps with plausible recommendations. We also discuss the WF on Tor studies' contribution category and development phase.}
}
@incollection{CUEVAS2025147,
title = {7 - Basic models},
editor = {Erik Cuevas and Karla Avila and Miguel Islas Toski and Héctor Escobar},
booktitle = {Agent-Based Models with MATLAB},
publisher = {Morgan Kaufmann},
pages = {147-232},
year = {2025},
isbn = {978-0-443-24004-1},
doi = {https://doi.org/10.1016/B978-0-443-24004-1.00007-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443240041000072},
author = {Erik Cuevas and Karla Avila and Miguel Islas Toski and Héctor Escobar},
keywords = {Agent-based models, Classic models, Neighborhood, Principles, Randomness},
abstract = {In this chapter, we will delve into the art of constructing agent-based models (Helbing, 2012). Agent-based modeling is a powerful and versatile tool that allows us to simulate complex systems by representing individual agents and their interactions within a given environment. Here, we will take a step-by-step approach, starting with classic models and gradually moving toward creating customized versions. By understanding how these models work and learning how to modify and extend them, we will gain valuable insights into alternative scenarios and even uncover entirely new phenomena.}
}
@incollection{ZEYER2015235,
title = {11 - For the mutual benefit: Health information provision in the science classroom},
editor = {Catherine {Arnott Smith} and Alla Keselman},
booktitle = {Meeting Health Information Needs Outside Of Healthcare},
publisher = {Chandos Publishing},
pages = {235-261},
year = {2015},
isbn = {978-0-08-100248-3},
doi = {https://doi.org/10.1016/B978-0-08-100248-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002483000111},
author = {Albert Zeyer and Daniel M. Levin and Alla Keselman},
keywords = {Health education, Health literacy, Information, Knowledge, Science education},
abstract = {In this chapter, the authors argue that the school science classroom should help students deal with complex real-life information about health and disease. They also discuss means by which curriculum and instruction in science education can be tied to these issues. The chapter reviews opportunities and challenges presented to individuals by the expectations of participatory health care, focusing on models of health literacy that can help understand and address the challenges. The authors argue that the problem of ensuring effective information use often lies in a transmission approach to health information provision. Transmitted knowledge is often not understood nor applied, as demonstrated in studies of human papillomavirus vaccination education. An alternative to knowledge transmission is the approach that aims to foster critical literacy, which is grounded in critical thinking essential to the practice of science. The chapter reviews a number of interdisciplinary science education activities that introduce health issues in the context of biology, physics, and chemistry education, ensuring deep understanding needed for developing critical literacy. It also discusses science education approaches and theories that encourage the development of deep, culturally meaningful science knowledge. Finally, the chapter reviews professional development and the role of various professionals, including teachers and librarians, in the collaborative endeavor of effective health information provision.}
}
@incollection{2020729,
title = {Index},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {729-744},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-815614-8.18001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156148180016}
}
@article{WAITE2020103838,
title = {Difficulties with design: The challenges of teaching design in K-5 programming},
journal = {Computers & Education},
volume = {150},
pages = {103838},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103838},
url = {https://www.sciencedirect.com/science/article/pii/S0360131520300385},
author = {Jane Waite and Paul Curzon and William Marsh and Sue Sentance},
keywords = {K-5 computing education, Teachers, Design, Programming},
abstract = {Teachers in England are required to ensure that learners from the age of five are taught about algorithms and program design. Yet, there is evidence that despite teachers reporting that design is important, they are not converting this into classroom practice. This paper describes a survey study, in which we explored teachers’ difficulties in using design. We surveyed 207 teachers asking them free-text questions on their use of design in teaching programming and their views of pupils’ responses to using design. In the survey, we also investigated teachers’ understanding of the term algorithm, an essential concept which may be a contributing factor in their difficulties with design. We provide underpinning data on the difficulties of using design that teachers of pupils aged from 5 to 11 years old (Grades K to 5) have in teaching programming. Difficulties with design identified include pupil resistance, a lack of time, a lack of pupil and teacher expertise, conflicting pedagogical choices and a general confusion over what an algorithm is. There were statistically significant differences in selection of the term ‘algorithm’ to describe programming artefacts whether a teacher was a specialist or a generalist, what training they had received on programming or design, the age group taught and programming language used. Teachers were more likely to call a complex code snippet an ‘algorithm’ than a simpler one and more likely to select the term to describe code snippets than a design artefact. We make suggestions of how to alleviate the problems including that teachers are introduced to the idea of ambiguous representations of algorithms and a process which refines the representation from ambiguous to unambiguous as the design progresses.}
}
@article{AOKI2013310,
title = {Propagation & level: Factors influencing in the ICT composite index at the school level},
journal = {Computers & Education},
volume = {60},
number = {1},
pages = {310-324},
year = {2013},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512001741},
author = {Hiroyuki Aoki and JaMee Kim and WonGyu Lee},
keywords = {ICT level, ICT composite index, Media in education, Pedagogical issue,  component and  component},
abstract = {Many nations are greatly affected by their education policies, and the educational level of different schools is relevant to a nation’s ICT policy. In the area of ICT, Korea has achieved quite high levels of competency. This study analyzed the level of ICT competency of 4490 elementary and 2419 middle schools in Korea within the context of the Korean educational system and social circumstances. The findings are as follows: first, differences in ICT level were greater among elementary schools than among middle schools; and secondly, ICT usage had a great impact on the ICT composite index for both elementary and middle schools. For both elementary and middle schools, the indicators that were found to have the greatest impact on the ICT composite index were ‘effort to make use of computers and ICT resources’ and ‘teaching of ICT-related subjects.’ Another variable that affected ICT competency was the level of ICT competency among the teachers and their willingness to use ICT in their lessons. This study found that merely the building of an ICT infrastructure is not enough to enhance the ICT level of schools. In addition to an ICT infrastructure the efforts of the teachers and administration were more important than any other factors. The findings of this study might provide useful suggestions to other nations that are endeavoring to enhance the ICT levels of their schools.}
}
@incollection{CAIRNS19863,
title = {CHAPTER 1 - A Contemporary Perspective on Social Development},
editor = {Phillip S. Strain and Michael J. Guralnick and Hill M. Walker},
booktitle = {Children's Social Behavior},
publisher = {Academic Press},
pages = {3-47},
year = {1986},
isbn = {978-0-12-673455-3},
doi = {https://doi.org/10.1016/B978-0-12-673455-3.50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780126734553500051},
author = {Robert B. Cairns}
}
@article{ADORNI2024100466,
title = {Development of algorithmic thinking skills in K-12 education: A comparative study of unplugged and digital assessment instruments},
journal = {Computers in Human Behavior Reports},
volume = {15},
pages = {100466},
year = {2024},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100466},
url = {https://www.sciencedirect.com/science/article/pii/S245195882400099X},
author = {Giorgia Adorni and Igor Artico and Alberto Piatti and Elia Lutz and Luca Maria Gambardella and Lucio Negrini and Francesco Mondada and Dorit Assaf},
keywords = {Teaching/learning strategies, 21st century abilities, Evaluation methodologies, Elementary education, Secondary education},
abstract = {In the rapidly evolving landscape of digital competencies, the need for a robust and universal method to assess students’ algorithmic thinking (AT) skills has become increasingly pronounced. Algorithmic thinking refers to the ability to analyse a problem and develop a step-by-step process to solve it. This research investigates the efficacy of the Cross Array Task (CAT) as an assessment tool for AT skills within Switzerland’s compulsory education system. Originally conceptualised as an unplugged activity, where students performed the task without digital technologies (e.g., by using gestures on paper) and an administrator manually assessed them, the CAT evolved into a digital activity that runs on an iPad. The CAT’s digital transformation has automated the scoring of student responses and data collection, streamlining the assessment processes and facilitating efficient large-scale assessments. It has also enhanced scalability, making the CAT suitable for widespread use in educational settings. Furthermore, it provides immediate feedback to students and educators, supporting timely interventions and personalised learning experiences. Our study aims to comprehensively investigate algorithmic competencies in compulsory education, examining their variations and influencing factors. This research examines key variables, such as age, sex, educational environment and school characteristics (e.g., the level and grade of education), and regional factors (e.g., the canton of the school) in Switzerland, and characteristics related to the specific assessment tool, including the type of artefact used, the complexity of the algorithms generated, and the level of autonomy. Additionally, it seeks to analyse the effectiveness of the unplugged and digital approaches in assessing AT skills, specifically comparing the unplugged and virtual CAT versions, aiming to provide insights into their advantages and potential synergies. This investigation delineates the developmental progression of AT skills across compulsory education, emphasising the influence of age on algorithm development and problem-solving strategies. Furthermore, we reveal the impact of artefacts and the potential of digital tools to facilitate advanced AT skill development across diverse age groups. Finally, our investigation delves into the influence of school environments and sex disparities on AT performance, alongside the significant individual variability influenced by personal abilities and external circumstances. These findings underscore the importance of tailored educational interventions and equitable practices to accommodate diverse learning profiles and optimise student outcomes in AT across educational settings.}
}
@article{CANNAVO2020102450,
title = {A visual editing tool supporting the production of 3D interactive graphics assets for public exhibitions},
journal = {International Journal of Human-Computer Studies},
volume = {141},
pages = {102450},
year = {2020},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2020.102450},
url = {https://www.sciencedirect.com/science/article/pii/S1071581920300525},
author = {Alberto Cannavò and Francesco De Pace and Federico Salaroglio and Fabrizio Lamberti},
keywords = {End-user development (EUD), Visual programming languages (VPLs), Interactive assets, 3D Graphics, Virtual reality (VR), Augmented reality (AR), Human-machine interaction (HMI), Natural user interfaces (NUIs)},
abstract = {The introduction of interactive assets in public exhibitions is capable to significantly enhance the visitors’ user experience. However, the creation of interactive applications could represent a challenging task, especially for users lacking computer skills. Visual programming languages (VPLs) – one of the instruments belonging to the broad categories of methods and tools devised to support end-user development (EUD) – promise to offer an intuitive way to overcome these limitations, by providing easy-to-use and efficient interfaces for encoding applications’ logic. Moving from these considerations, this paper first analyses pros and cons of tools devised so far to support the generation of interactive contents. Then, it presents the design of a new tool named Visual Scene Editor (VSE), which allows users with little to no programming skills to create 3D interactive applications by combining available assets through an interactive, visual process. Both objective and subjective measurements have been collected with both skilled and unskilled users to evaluate the performance of the proposed tool. A comparison with existing solutions shows a reduction in the time required to complete the assigned tasks, of the complexity of the logic created, as well as of the number of errors made, confirming the suitability of the VSE for the said purpose.}
}
@article{MCCART2013235,
title = {Goal attainment on long tail web sites: An information foraging approach},
journal = {Decision Support Systems},
volume = {55},
number = {1},
pages = {235-246},
year = {2013},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2013.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167923613000535},
author = {J.A. McCart and B. Padmanabhan and D.J. Berndt},
keywords = {Information Foraging Theory, Long tail, Data mining, Clickstream analysis},
abstract = {The long tail has attracted substantial theoretical as well as practical interest, yet there have been few empirical studies that have explicitly examined the factors that drive online conversions at these sites. This research tests several hypotheses derived from Information Foraging Theory (IFT) that pertain to goal achievement on long tail Web sites. IFT introduced concepts of information patches and information scent to model information seeking behavior of individuals, but has mostly been tested in production rule environments where the theory is used to simulate user behavior. Testing IFT-driven hypotheses on real data required learning information patches and scents using an inductive approach and in this paper we adapt existing algorithms for these discovery tasks. Our results based on clickstream data from forty-seven small business Web sites show both the existence of valuable information patches and information scent trails as well as their importance in explaining conversion on these sites. The majority of the hypotheses were supported and we discuss the implications of this for researchers and practitioners.}
}
@article{WEINTROP2019103646,
title = {Transitioning from introductory block-based and text-based environments to professional programming languages in high school computer science classrooms},
journal = {Computers & Education},
volume = {142},
pages = {103646},
year = {2019},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.103646},
url = {https://www.sciencedirect.com/science/article/pii/S036013151930199X},
author = {David Weintrop and Uri Wilensky},
keywords = {Evaluation of CAL systems, Interactive learning environments, Programming and programming languages, Secondary education, Teaching/learning strategies},
abstract = {Block-based programming languages are becoming increasingly common in introductory computer science classrooms across the K-12 spectrum. One justification for the use of block-based environments in formal educational settings is the idea that the concepts and practices developed using these introductory tools will prepare learners for future computer science learning opportunities. This view is built on the assumption that the attitudinal and conceptual learning gains made while working in the introductory block-based environments will transfer to conventional text-based programming languages. To test this hypothesis, this paper presents the results of a quasi-experimental classroom study in which programming novices spent five-week using either a block-based or text-based programming environment. After five weeks in the introductory tool, students transitioned to Java, a conventional text-based programming language. The study followed students for 10 weeks after the transition. Over the course of the 15-week study, attitudinal and conceptual assessments were administered and student-authored programs were collected. Conceptual learning, attitudinal shifts, and changes in programming practices were analyzed to evaluate how introductory modality impacted learners as they transitioned to a professional, text-based programming language. The findings from this study build on earlier work that found a difference in performance on content assessments after the introductory portion of the study (Weintrop & Wilensky, 2017a). This paper shows the difference in conceptual learning that emerged after five weeks between the block-based and text-based conditions fades after 10 weeks in Java. No differences in programming practices were found between the two conditions while working in Java. Likewise, differences in attitudinal measures that emerged after working in the introductory environments also faded after 10 weeks in Java, resulting in no difference between the conditions after 15 weeks. The contribution of this work is to advance our understanding of the benefits and limits of block-based programming tools in preparing students for future computer science learning. This paper presents the first quasi-experimental study of the transfer of knowledge between block-based and text-based environments in a high school setting. The lack of significant differences between the two introductory programming modalities after learners transition to professional programming languages is discussed along with the implications of these findings for computer science education researchers and educators, as well as for the broader community of researchers studying the role of technology in education.}
}
@incollection{FISHWICK2017557,
title = {Chapter 21 - Aesthetic Computing*},
editor = {Myounghoon Jeon},
booktitle = {Emotions and Affect in Human Factors and Human-Computer Interaction},
publisher = {Academic Press},
address = {San Diego},
pages = {557-587},
year = {2017},
isbn = {978-0-12-801851-4},
doi = {https://doi.org/10.1016/B978-0-12-801851-4.00021-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128018514000215},
author = {Paul A. Fishwick}
}
@incollection{KIRK2013235,
title = {Chapter 11 - Application Case Study: Advanced MRI Reconstruction},
editor = {David B. Kirk and Wen-mei W. Hwu},
booktitle = {Programming Massively Parallel Processors (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {235-264},
year = {2013},
isbn = {978-0-12-415992-1},
doi = {https://doi.org/10.1016/B978-0-12-415992-1.00011-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124159921000110},
author = {David B. Kirk and Wen-mei W. Hwu}
}
@article{ROSSON2008468,
title = {Design planning by end-user web developers},
journal = {Journal of Visual Languages & Computing},
volume = {19},
number = {4},
pages = {468-484},
year = {2008},
note = {Selected Papers from IEEE Symposium on Visual Languages and Human Centric Computing 2007 (VL/HCC 2007)},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2008.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X08000141},
author = {Mary Beth Rosson and Hansa Sinha and Mithu Bhattacharya and Dejin Zhao},
keywords = {End-user programming, Web development, Design, Concept maps},
abstract = {We report an exploratory research project that investigates the impacts of different forms of design planning on end users asked to develop a simple interactive web application. End users created their projects (a Ride Board application) using the CLICK end-user web development tool [J. Rode, User-centered design of end-user web development tool, Ph.D. Dissertation, Department of Computer Science, Virginia Tech, Blacksburg, VA, USA, 2005]. Some participants were asked to create a conceptual map to plan their projects and others to write user interaction scenarios; a third group was asked to do whatever they found useful. We describe the planning that each group underwent, how they approached the web development task, and their reactions to the experience afterwards. The overall pattern of results suggests that while the participants who planned using scenarios felt they better understood the web development task, it was the group who created concept maps that explored and incorporated more of the novel programming features of the CLICK tool. We also discuss the role of gender in the CLICK development task, noting that women were less likely to explore the tool's novel features and perceived themselves as less successful in the task. We conclude with a discussion of design implications and future work.}
}
@incollection{2024552,
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {552-598},
year = {2024},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.09001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895090015}
}
@article{HUANG2024152294,
title = {Machine learning in energy storage material discovery and performance prediction},
journal = {Chemical Engineering Journal},
volume = {492},
pages = {152294},
year = {2024},
issn = {1385-8947},
doi = {https://doi.org/10.1016/j.cej.2024.152294},
url = {https://www.sciencedirect.com/science/article/pii/S1385894724037811},
author = {Guochang Huang and Fuqiang Huang and Wujie Dong},
abstract = {Energy storage material is one of the critical materials in modern life. However, due to the difficulty of material development, the existing mainstream batteries still use the materials system developed decades ago. Machine learning (ML) is rapidly changing the paradigm of energy storage material discovery and performance prediction due to its ability to solve complex problems efficiently and automatically. Various excellent works are constantly emerging in the field of ML assisted or dominated development of energy storage material, such as exploring of new materials, studying of battery performance, investigating of battery aging mechanism. In this paper, we methodically review recent advances in discovery and performance prediction of energy storage materials relying on ML. After a brief introduction to the general workflow of ML, we provide an overview of the current status and dilemmas of ML databases commonly used in energy storage materials. The typical applications and examples of ML to the finding of novel energy storage materials and the performance forecasting of electrode and electrolyte materials. Furthermore, we explore the dilemmas that will be faced in the development of applied ML-assisted or dominated energy storage materials and propose a corresponding outlook. This review systematically summarizes the current development of ML-assisted energy storage materials research, which is expected to point the way for its further development.}
}
@article{WANG2017182,
title = {Bee and Frog Co-Evolution Algorithm and its application},
journal = {Applied Soft Computing},
volume = {56},
pages = {182-198},
year = {2017},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2017.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S1568494617301096},
author = {Hong-bo Wang and Xue-Na Ren and Xu-yan Tu},
keywords = {Artificial Bee Colony, Shuffled Frog Leaping, Evolutionary computing, Resources scheduling},
abstract = {In order to obtain better generalization abilities and mitigate the impacts of the best and worst individuals during the process of optimization, this paper suggests Bee and Frog Co-Evolution Algorithm (abbreviation for BFCEA), which combines Mnemonic Shuffled Frog Leaping algorithm With Cooperation and Mutation (abbreviation for MSFLACM) with improved Artificial Bee Colony (abbreviation for ABC). The contrast experimental study about different iteratively updating strategies was acted in BFCEA, including strategy of integrating with ABC, regeneration of the worst frog and its leaping step. The key techniques focus on the first 10 and the last 10 frogs evolving ABC in BFCEA, namely, the synchronous renewal strategy for those winner and loser should be applied, after certain G times’ MSFLACM-running, so as to avoid trapping local optimum in later stage. The ABC evolution process will be called between all memes’ completing inner iteration and all frogs’ outer shuffling, the crossover operation is removed from MSFLACM for its little effect on time-consuming and convergence in this novel algorithm. Besides, in ABC, the scout bee is generated by Cauchy mutating instead at random. The performance of proposed approach is examined by well-known 16 numerical benchmark functions, and obtained results are compared with basic Shuffled Frog Leaping algorithm (abbreviation for SFLA), ABC and four other variants. The experimental results and related application in cloud resource scheduling show that the proposed algorithm is effective and outperforms other variants, in terms of solution quality and convergence, and the improved variants can obtain a lower degree of unbalanced load and relatively stable scheduling strategy of resources in complicated cloud computing environment.}
}
@incollection{DIETRICH19943,
title = {CHAPTER 1 - Thinking Computers and The Problem of Intentionality},
editor = {Eric Dietrich},
booktitle = {Thinking Computers and Virtual Persons},
publisher = {Academic Press},
pages = {3-34},
year = {1994},
isbn = {978-0-12-215495-9},
doi = {https://doi.org/10.1016/B978-0-12-215495-9.50006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780122154959500065},
author = {Eric Dietrich},
abstract = {Publisher Summary
This chapter is an attempt to clear the names of artificial intelligence (AI) and computational cognitive science. These two related disciplines have been accused of a conceptual error so profound that their very existence is jeopardized. Sometimes, however, philosophers successfully arrest and lock up the guilty. The best example of this, ironically, is in psychology. Artificial intelligence and computational cognitive science are both committed to the claim that computers can think. The former is committed to the claim that human-made computers can think, while computational cognitive science is committed to the view that naturally occurring computers, brains, think. AI is the field dedicated to building intelligent computers. AI ultimately wants a machine that could solve very difficult, novel problems like proving Fermat's last theorem, correcting the greenhouse effect, or figuring out the fundamental structure of space-time. Historically, AI is associated with computer science, but the compleat AI researcher frequently knows a fair amount of psychology, linguistics, neuroscience, mathematics, and possibly some other discipline.}
}
@article{PINSKI2024100062,
title = {AI literacy for users – A comprehensive review and future research directions of learning methods, components, and effects},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100062},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100062},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000227},
author = {Marc Pinski and Alexander Benlian},
keywords = {Systematic literature review, Scoping literature review, Artificial intelligence literacy, Learning methods, AI literacy components, AI literacy effects},
abstract = {The rapid advancement of artificial intelligence (AI) has brought transformative changes to various aspects of human life, leading to an exponential increase in the number of AI users. The broad access and usage of AI enable immense benefits but also give rise to significant challenges. One way for AI users to address these challenges is to develop AI literacy, referring to human proficiency in different subject areas of AI that enable purposeful, efficient, and ethical usage of AI technologies. This study aims to comprehensively understand and structure the research on AI literacy for AI users through a systematic, scoping literature review. Therefore, we synthesize the literature, provide a conceptual framework, and develop a research agenda. Our review paper holistically assesses the fragmented AI literacy research landscape (68 papers) while critically examining its specificity to different user groups and its distinction from other technology literacies, exposing that research efforts are partly not well integrated. We organize our findings in an overarching conceptual framework structured along the learning methods leading to, the components constituting, and the effects stemming from AI literacy. Our research agenda – oriented along the developed conceptual framework – sheds light on the most promising research opportunities to prepare AI users for an AI-powered future of work and society.}
}
@article{KUGEL1986137,
title = {Thinking may be more than computing},
journal = {Cognition},
volume = {22},
number = {2},
pages = {137-198},
year = {1986},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(86)90057-0},
url = {https://www.sciencedirect.com/science/article/pii/0010027786900570},
author = {Peter Kugel},
abstract = {The uncomputable parts of thinking (if there are any) can be studied in much the same spirit that Turing (1950) suggested for the study of its computable parts. We can develop precise accounts of cognitive processes that, although they involve more than computing, can still be modelled on the machines we call ‘computers’. In this paper, I want to suggest some ways that this might be done, using ideas from the mathematical theory of uncomputability (or Recursion Theory). And I want to suggest some uses to which the resulting models might be put. (The reader more interested in the models and their uses than the mathematics and its theorems, might want to skim or skip the mathematical parts.)
Résumé
Les éléments du raisonnement ne relevant pas du calculable (uncomputable), (s'il en existe), peuvent s'etudier dans I'optique suggérée par Turing (1950) pour l'étude des éléments calculables (computable). On peut rendre compte avec précision des processus cognitifs qui, bien qu'impliquant plus que des calculs, peuvent cependant être modélisés sur ordinateurs. Dans cet article l'auteur propose des modalités pour arriver à ces résultats en utilisant les idées de la théorie mathdmatique de la Récursion (uncomputability). L'auteur suggère aussi des utilisations pour les modéles que en découlent (Il est possible au lecteur plus intéressé par les modèles et leurs utilisations que par les mathématiques et les théorèmes de passer rapidement sur la partie mathématique ou d'omettre de la lire.)}
}
@incollection{KIRK2017305,
title = {Chapter 14 - Application case study—non-Cartesian magnetic resonance imaging: An introduction to statistical estimation methods},
editor = {David B. Kirk and Wen-mei W. Hwu},
booktitle = {Programming Massively Parallel Processors (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
pages = {305-329},
year = {2017},
isbn = {978-0-12-811986-0},
doi = {https://doi.org/10.1016/B978-0-12-811986-0.00014-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119860000145},
author = {David B. Kirk and Wen-mei W. Hwu},
keywords = {Statistical estimation methods, matrix–vector multiplication, linear solvers, iterative methods, MRI, non-Cartesian scan trajectory, error bounds, signal-to-noise ratio, trigonometry functions, floating-point precision and accuracy},
abstract = {This chapter presents an application study on using CUDA C and GPU computing to accelerate an iterative solver for reconstruction of an MRI image from Non-Cartesian scan data. It covers the process of identifying the appropriate type of parallelism, loop transformations, mapping data into constant memory, mapping data into registers, data layout transformations, using special hardware instructions, and experimental tuning. It also demonstrates a process of validating the design choices with domain-specific criteria.}
}
@article{MARKAUSKAITE2022100056,
title = {Rethinking the entwinement between artificial intelligence and human learning: What capabilities do learners need for a world with AI?},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100056},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100056},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X2200011X},
author = {Lina Markauskaite and Rebecca Marrone and Oleksandra Poquet and Simon Knight and Roberto Martinez-Maldonado and Sarah Howard and Jo Tondeur and Maarten {De Laat} and Simon {Buckingham Shum} and Dragan Gašević and George Siemens},
keywords = {Capabilities for AI, AI in education, Postdigital dialogue, Ecological approach},
abstract = {The proliferation of AI in many aspects of human life—from personal leisure, to collaborative professional work, to global policy decisions—poses a sharp question about how to prepare people for an interconnected, fast-changing world which is increasingly becoming saturated with technological devices and agentic machines. What kinds of capabilities do people need in a world infused with AI? How can we conceptualise these capabilities? How can we help learners develop them? How can we empirically study and assess their development? With this paper, we open the discussion by adopting a dialogical knowledge-making approach. Our team of 11 co-authors participated in an orchestrated written discussion. Engaging in a semi-independent and semi-joint written polylogue, we assembled a pool of ideas of what these capabilities are and how learners could be helped to develop them. Simultaneously, we discussed conceptual and methodological ideas that would enable us to test and refine our hypothetical views. In synthesising these ideas, we propose that there is a need to move beyond AI-centred views of capabilities and consider the ecology of technology, cognition, social interaction, and values.}
}
@incollection{2017535,
title = {Index},
editor = {David B. Kirk and Wen-mei W. Hwu},
booktitle = {Programming Massively Parallel Processors (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
pages = {535-550},
year = {2017},
isbn = {978-0-12-811986-0},
doi = {https://doi.org/10.1016/B978-0-12-811986-0.00038-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119860000388}
}
@incollection{FREWIN2012209,
title = {Chapter 6 - Biocompatibility of SiC for Neurological Applications},
editor = {Stephen E. Saddow},
booktitle = {Silicon Carbide Biotechnology},
publisher = {Elsevier},
address = {Oxford},
pages = {209-256},
year = {2012},
isbn = {978-0-12-385906-8},
doi = {https://doi.org/10.1016/B978-0-12-385906-8.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123859068000064},
author = {Christopher L. Frewin and Chris Locke and Stephen E. Saddow and Edwin J. Weeber},
keywords = {Central nervous system, glial scarring, biocompatibility, in vitro, PC12, H4, neuron, MTT assay, live cell atomic force microscopy, silicon, 3C-SiC, nanocrystalline diamond, in vivo, long-term implantation},
abstract = {Publisher Summary
This chapter introduces the research that done at the University of South Florida to determine the biocompatibility of single-crystal SiC and related materials with neuronal and glial cells to determine whether they are possible candidate materials for the construction of a neuronal implantation device. It describes the basics about the CNS, how it is organized, and its major cellular constituents are described and their purposes within the CNS. Following this, it considers the research investigating the in vitro biocompatibility of 3C-SiC and a related material, nanocrystalline diamond (NCD), with immortalized neuronal and CNS cell lines. The in vitro reaction of primary derived neurons with 3C-SiC is also detailed, followed by initial in vivo testing of 3C-SiC within the brain of a wild-type mouse. The central nervous system (CNS) is composed of the brain and spinal cord. Blood is not in direct contact with the extracellular fluid within the CNS because of restrictive endothelial cells that create a tight junction known as the blood–brain barrier. Any physical biomedical device that would interact with the CNS has to deal with cells that perform functions similar to their leukocyte counterparts found within the bloodstream. The therapeutic utility provided by these devices is that they could be utilized as a platform for drug delivery behind the blood–brain barrier to specific CNS areas, or to transport neuronal factors or even other cells to the CNS to assist in the repair of the neural system damaged by trauma and disease.}
}